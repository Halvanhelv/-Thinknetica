Unicorn быстрее поскольку  он сильно завязан на операционной система unix к примеру данные между прокси сервером и сервером приложений проходят не через tcp/ip а через unix socket
так же он использует signals для меж проццесорного взаимодействия(это такая концепция в unix которая позволяет посылать некие команды процессу к примеру команды завершения процесса плсле того как процесс завершит свою работу или немедленное завершение вне зависимости от того закончил процесс свою работу или нет) и так же unicorn использует распределение нагрузки между своими процессами(воркерами) средствами операционной системы, так же Unicorn перезагружает процесс если он завис и от него долго нет ответа то unicorn останавливает этот процесс и запускает новый, так же Unicorn(так же puma и passenger(в платной версии)) позволяет делать так называемы "zero-downtime" deploy то есть деплой с нулевым уровнем простоя, допустим если мы деплой через passenger то во время деплоя нужно вызвать команду стоп что бы passenger остновился и конечно в это время приложение недоступно и после деплоя мы снова запускаем passenger и в это время у нас происходит простой и приложение недоступно, unicorn делает это прозрачно в фоне и пока приложение до тех пор пока не запушена новая версия(деплой полностью не завершен) работает старая версия все время

https://i.imgur.com/pvGjvju.png

1)запрос приходит в front-end сервер(nginx) или в load-balancer в качестве которого выступает nginx или другой прокси сервер

2)дальше nginx отправляет запрос в shared socket(это концепция для обмена данными на уровне *nix систем), все запросы встают в очередь, дальше за этой очередью следит master процесс Unicorn-a, master процесс в unicorn один, этот мастер процесс запускает новые воркеты(workers), воркеры это копии нашего приложения которые занимаются обработкой этого запроса, то есть это копия Rails приложения, master процесс следит за очередью за тем что бы она была доступна и создает по необходимости новые worker процессы, после того как запушены worker процессы они так же начинают следить за очередью и как только в очереди появляет некий запрос они его берут на обработку за счет этого очень маленький простой между попаданием запроса в очередь и его обработкой потому что как только какой то worker осводобился он смотрит нет ли запросов в очереди и пытается его обработать, так же master следит как долго некий worker обрабатывает запрос и если это будет достаточно долго то этот worker завершает свою работу то есть master процесс посылает ему signal на завершение и запускается новый worker который берет запрос в обработку, если запросов мало то master процесс уменьшает количество воркеров а если много то master процесс по опреденному алгоритму увеличивает количество воркеров, то есть основная задача master процесса это запускать воркеры и следить за их работой 

Zero-downtime deploy работает по следующему принципу 
Первый worker созданный новым master процессов посылает сигнал старому master процессу и тот завершается и завершает свои worker процессы при этом давая время на обработку тех запросов которые на данный момент еще обрабатываются старыми воркерами, это так называемы graceful downtime или graceful stoppng или вежливая остановка а не мнгновенная остановка запросов что бы пользователь который послал запрос все таки получил ответ от старой копии кода(от воркера старого master процесса), после того как все старые воркеры завершены то завершается и старый master процессс и остается только один новый master процесс и пользователи обращаются к обновленному приложению, все это происходит очень быстро

https://i.imgur.com/qGhgRkL.png


Passenger так же поддерживает zero-timedown deployment но только в платной версии, сервер Puma так же поддерживает  zero-timedown deployment и средних проектах разницы между сервера приложений не будет заметна


Мы рассмотрели Passenger а теперь рассмотрит настройку Unicorn, Puma легка в настройке и рассматривать не будем( по сути надо просто правильно скогфигурировать nginx что бы уже делали)

 Переведем сервер с Passenger на Unicorn, когда мы настраивали Passenger то делали это на сервере, в самом приложении ничего настраивать и дополнительно писать ничего не нужно было
 а в Unicorn нужно конфиг держать внутри нашего проекта а так же поставить гем Unicorn что бы наше приложение умело запускать Unicorn


 Нужно поставить gem 'unicorn' и 

 gem 'capistrano3-unicorn', require: false

 и в Capfile добавляем require 'capistrano3/unicorn'

 и закоментируем require "capistrano/passenger"

 что бы не было набор задач для двух серверов, они будут конфликтовать но capistrano может делать лишнюю работу при деплое

 и в отличии от capistrano/passenger у capistrano3/unicorn нет автоматического рестарта сервера у потому в deploy.rb нужно вручную написать когда делать перезагружать сервер

 до того как capistrano выкатит полностью код на сервер и установить символьные ссылки нужно запустить команду

 after 'deploy:publishing', 'unicorn:restart'  

 команда 'unicorn:restart' появилась благодоря
  gem 'capistrano3-unicorn', require: false у самого unicorn довольно длинные и сложные команды для запуска и перезапуска и для упрашения нам помогает gem capistrano3/unicorn которые предоставляет задачи для capistrano 

  Теперь нужно написать конфиг для Unicorn и у каждой среды окружения(production staging и других) нужен свой конфиг файл так как у них буду разные пути к приложению, так как минимум они должны лежать в разных дерикториях если они запускаются на одном сервере

  создаем конфиг config/unicorn/production.rb

 и просто копируем из интернета готовый конфиг и немного подправим 
 ```
 # paths
app_path = "/home/deployer/qna" # дериктория всего проекта
working_directory "#{app_path}/current" # дериктория текущего рабочего проекта
pid               "#{app_path}/current/tmp/pids/unicorn.pid"  # путь где расположен файл содержит id процесса unicorn-a но которому capistrano или другие процессы запушен ли Unicorn и какой процесс нужно будет оставить при надобности и тд, 

# listen
listen "#{app_path}/shared/tmp/sockets/unicorn.qna.sock", backlog: 64 # путь к сокету(unix) для обмена данными так как unicorn не использует tcp/ip для общения между nginx и сервером приложения и важно что путь был таким что бы к нему имел доступ и наше приложение и nginx, опция backlog означает максимальное количество воркеров(иногда число будет меньше или больше от загруженности но unicorn будет стремиться не превышать 64), по умолчанию 1024

# logging
stderr_path "log/unicorn.stderr.log"  #логи ошибок Unicorn 
stdout_path "log/unicorn.stdout.log" #логи Unicorn

# workers 
worker_processes 2 #количество master процессов # ставить по количеству ядер и потоков, если много ОЗУ то можно поставить больше чем есть ядер но уже будет не истинная парарелльность а Вытесняющая многозадачность, но даже в минимальной конфигурации 1 ядро и 1 гб ОЗУ вполне можно поставить 2 воркер процесса, будет работать стабильно

# use correct Gemfile on restarts  # хук который выполняет перед тем как сервер запускается 
before_exec do |server|
  ENV['BUNDLE_GEMFILE'] = "#{app_path}/current/Gemfile" # что бы unicorn запускался с правильным набором гемов 
end

# preload
preload_app true # она отвечает за принцип загрузки приложений, без нее не будет работать zero-downtime deployment, прежде чем создать worker или master процессы unicorn загружает в ОЗУ все приложение, дальше стартует master процесс с уже загруженным приложением и когда master создает новые воркеры то воркер копирует приложение из памяти а не читает с диска и это гораздо более быстро на каждый worker старт приложения и общий старт приложения гораздо быстрее  работает и при  zero-downtime deployment позволяет осуществить переход на новую кодовую базу потому что в память загружается новая версия приложения и там еще содержится старая версия приложения и в какой то момент все новые воркеры начинают использовать  новую версию приложения 

before_fork do |server, worker| # выполняется перед тем как создается worker который будет непосредственно обрабатывать запрос 
  # the following is highly recomended for Rails + "preload_app true"
  # as there's no need for the master process to hold a connection
  if defined?(ActiveRecord::Base) # если в приложении используется ActiveRecord 
    ActiveRecord::Base.connection.disconnect! # то разрывается соедение с базой данных, это делается что бы master процесс не держал соеденение и поэтому когда создается новый worker сначала соеденение разрывается  а после того как worker соеденение снова устанавливается в блоке(ниже по тексту)
  end

  # Before forking, kill the master process that belongs to the .oldbin PID.
  # This enables 0 downtime deploys.
  old_pid = "#{server.config[:pid]}.oldbin" # Ищет файл с pid  и добавляет к названию oldbin
  if File.exists?(old_pid) && server.pid != old_pid # Если такой файл существует и Pid текущего запушенного сервера не равен тому что что в old_pid
    begin
      Process.kill("QUIT", File.read(old_pid).to_i) # то старому процессу посылается сообщение QUIT то есть процесс должен завершить все начатые операции и завершится сам 
    rescue Errno::ENOENT, Errno::ESRCH # если вонзикает проблема то возникат исключение и можно его обработать но лучше не трогать а код этот пишется что бы перехватить исключение и ничего с ним не делать 
      # someone else did our job for us
    end
  end
end

after_fork do |server, worker| # снова устанавливает соеденение с Active Record, если так не делать то при создании новых воркеров то могут быть конфликты между воркерами, то есть соединения переиспользуются и может оказатся одно соеденение быть занятым бругие воркером и новый worker попытается использовать то же самое соеденение и из за этого он не запуститься 
  if defined?(ActiveRecord::Base)
    ActiveRecord::Base.establish_connection
  end
end
 ```
Теперь на сервере надо изменить конфиг nginx что бы он работал с Unicorn

Заходим на удаленный сервер и открываем конфиг nginx который отвечает за настройку конкретно нашего приложения:
sudo nano /etc/nginx/sites-enabled/rails_app_name.conf
и нужно настроить upstream, это деректива nginx которая указывает список back-end(серверов приложений) серверов, если указать несколько то nginx будет работать как load balancer и nginx будет проксировать запрос по серверам которые мы там укажем  по очередно 

```nginx
{

upstream %any_name% {
	server unix:/home/%user%/%app_name%/shared/tmp/sockets/%unicorn_socket_name% fail_timeout=0 # список серверов на каждой строчке новый сервер, fail_timeout=0; значит что nginx не будет слать запрос если наш сервер приложений не отвечает 
	server unix:/home/%user%/%app_name%/shared/tmp/sockets/%unicorn_socket_name% fail_timeout=0 # список серверов на каждой строчке новый сервер
}

client_max_body_size 20M; # максимальный размер запроса, нужен что бы загружать файлы на сервер, настройка nginx и не относится ни к какому серверу приложений, если не #указать и размер запроса будет большой то nginx сообщит что размер запроса слишком большой, если настроена прямая загрузка файлов в облако то эта опция не нужна 
от того что мы указали upstream %any_name% запросы туда автоматом не пойдут, нужно явно указать

в начале указываем путь до статических файлов
try_files $uri/index.html $uri @%anyname%; # $uri это запрошенный относительный путь к нашему серверу без хоста, строчку следует читать как "если был запрошен $uri/index.html то отдаем его а если не найден путь $uri то обращаемся к @%anyname%" #@%anyname% это то что написано в строчке ниже, 
location @%anyname% { # это имя upstream указаное выше или проще говоря путь который мы указали в upstream
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # proxy_set_header - это nginx как прокси сервер будет устанавливать различные заголовки как к запросу так и к ответу,  X-Forwarded-For это заголовок,  а $proxy_add_x_forwarded_for поле заголовка запроса клиента “X-Forwarded-For”, все это нужно что бы видеть реальный ip адрес с которого сделан запрос поскольку если мы не укажем эту дерективу и этот заголовок то получится что от некоторого внешнего ip адреса пришел запрос к нашему серверу, его перехватил nginx и дальше мы будем видеть что запрос идет от локального ip адреса 127.0.0.1 но если поставить такой заголовок то nginx проксирует сам ip адрес клиента который делает запрос в наше back-end приложение и мы видим реальные ip адреса
proxy_set_header Host $http_host; # что бы так же проксировался хост который запрашивался а не подставлялось 127.0.0.1 
proxy_redirect off; запрещаем проксирование редиректов то есть если наш back-end вернет redirect то сам nginx его не делает он возвращает redirect браузеру клиента 
proxy_pass http://%any_name% # указываем имя upsteam
}
}

```
Сохраняем и проверяем что в конфиге нет ошибок sudo service nginx configtest и перезапускаем sudo service nginx restart
пушим все изменения в гитхаб и деплоим и если сразу после деплоя попробовать зайти на сайт то оно быстро откроектся блягодоря preload_app true чем это было в прошлый раз на passenger но на passenger так тоже можно настроить

в задачи капистрано так же добавились новые таски к примеру что бы увеличить количество воркеров на удаленном сервере, это гораздо удобнее чем делать это вручную и многое другое 

-----------------------------------------------------------------------------------------------------------------------------------------------------

Monit это система мониторинга которая переодически опрашивает процессы и перезапускает если процесс упал, так же можно выставлять правила, к примеру грузит CPU на 80% в течении последних 10 минут или он занимает больше 80% доступной ему памяти то лучше перезапустить, все это можно настроить для отслеживания в реальном времени и даже настроить отправку уведомления в случае определенных событий, что бы администратор был в курсе дел, 

ставится командой на удаленном сервере ```sudo apt install monit```  и проверим что он установился sudo service monit status а теперь сконфигурируем глобальный конфиг monit
  sudo nano /etc/monit/monitrc

  set daemon с какой переодичностью опрашивать процессы, есть еще много настроект, можно настроить smtp сервер что бы отправлять отчеты администратору, так же можно настроить формат самого сообщения
  monit включает в себя web интерфейс со встроенным веб сервером который позволяет за сервером через веб интерфейс, смотреть статусы процессов, запускать, перезапускать, включать, выключать мониторинг и тд
  https://i.imgur.com/idv0ndh.png

  строчка set http port 2812 включает интерфейс
  остальные строчки это разные примеры как получить доступ к интерфейсу
  allow login:password используя логин и пароль
  use address localhost получать доступ с определенного адресса
  так же есть способ настроить через ssl 

  так же в конце файла подключаются доп файлы в которых идет настройка уже индивидуального конфига monit https://i.imgur.com/5XiAXEg.png

  сохраняемся и перезагружаем sudo service monit restart
  Заходим на наш сайт с портом 2812 указанным в конфиге
  Видим что показывается только процесс нашего приложения qna, нужно добавить и другие процессы 
  Теперь создаем конфиг в корне приложения qna что бы он был всегда рядом с приложением на случай если нужен будет на другой машине  c со следующим содержанием :

  ```

### Nginx ###
check process nginx with pidfile /run/nginx.pid # любое правило начинается с ключевых слов check proccess дальше любое название для процесса, with pifile путь к pid файлу процесса именно по нему monit определяет есть такой процесс, сколько он отнимает памятина сколько нагружает CPU
  start program = "/usr/sbin/service nginx start" #  абсолютный путь до нужной команды, судо не нужно использовать так как monit работает под root пользователем, с помощью whick service можно узнать путь до команды
  stop program = "/usr/sbin/service nginx stop"
  # дальше идут необязательные правила
  if cpu > 60% for 2 cycles then alert # cycles это переодичность указанная в главном конфиге monit, alert это отправка уведомления администратору если настроен smtp и формат сообщения в конфиге
  if cpu > 80% for 5 cycles then restarts 
  if memory usage > 80% for 5 cycles then restart
  if failed host 188.166.44.84 port 80 protocol http # если до удаленного сервера не идут запросы то перезагрузить nginx
    then restart
  if 3 restarts within 5 cycles then timeout # если было больше 5 перезагрузок то monit отключает мониторинг и правила выше больше не работают
  
### Postgresql ###
check process postgresql
  with pidfile /var/run/postgresql/10-main.pid
  start program = "/usr/sbin/service postgresql start"
  stop  program = "/usr/sbin/service postgresql stop"
  if failed host localhost port 5432 protocol pgsql then restart # если не отвечает postgres с протоколом pgsql
  if cpu > 80% then restart
  if memory usage > 80% for 2 cycles then restart
  if 5 restarts within 5 cycles then timeout

### Redis ###
check process redis-server
    with pidfile "/var/run/redis/redis-server.pid"
    start program = "/usr/sbin/service redis start"
    stop program = "/usr/sbin/service redis stop"
    if totalmem > 100 Mb then alert # общее количество памяти которое занимает процесс, так как redis имеет структуру форков, то есть он имеет master процесс и форки
    if children > 255 for 5 cycles then stop # если количество дочерних процессов больше чем 255 в течении 5 циклов то останавливаем
    if cpu usage > 95% for 3 cycles then restart
    if memory usage > 80% for 5 cycles then restart
    if failed host 127.0.0.1 port 6379 then restart
    if 5 restarts within 5 cycles then timeout

### Sidekiq ###
check process sidekiq
  with pidfile "/home/deployer/qna/shared/tmp/pids/sidekiq-0.pid"
  # так как monit работает по root пользователем то там нет rvm, она должна вызывать от имени linux пользователя(deployer в данном случае) на котором стоит rvm
/bin/su - deployer это переключение на нужного пользователя
-c это флаг что бы можно было ввести команду  "cd /home/........."
мы можем узнать откуда все эти команды взглянув на логи capistrano и там будет указано какими командами он запускает и останавливает процессы, если название процесса не помешается в экран то сохраняем лог команды в файл к примеру cap production sidekiq:stop > /tmp/file.log и иногда команда может запускать сразу несколько процесс помимо start и stop но вычленяем только нужную команду, если нет pid файла то расскоментируйте строчку в файле sudo nano /home/deployer/.config/systemd/user/sidekiq.service, файл который мы настраивали в прошлом конспекте, внимательно читаем статью и в конфиг файле будут комментарии для ясности http://ilab.me/howto/run-sidekiq-6-using-systemd/
  start program = "/bin/su - deployer -c 'cd /home/deployer/qna/current && /home/deployer/.rvm/bin/rvm default do bundle exec sidekiq --index 0 --pidfile /home/deployer/qna/shared/tmp/pids/sidekiq-0.pid --environment production --logfile /home/deployer/qna/shared/log/sidekiq.log --daemon'"




  stop program = "/bin/su - deployer -c 'cd /home/deployer/qna/current && /home/deployer/.rvm/bin/rvm default do bundle exec sidekiqctl stop /home/deployer/qna/shared/tmp/pids/sidekiq-0.pid 10'"
  if cpu > 80% then restart
  if memory usage > 80% for 2 cycles then restart
  if 3 restarts within 3 cycles then timeout
  ```
